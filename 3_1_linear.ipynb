{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95ba2149-d657-4fed-b44c-2dfb8d8f929a",
   "metadata": {
    "origin_pos": 0,
    "tags": []
   },
   "source": [
    "# Linear Regression\n",
    "\n",
    "### Linear Model\n",
    "\n",
    "For a collection of features $\\mathbf{X}$,\n",
    "the predictions $\\hat{\\mathbf{y}} \\in \\mathbb{R}^n$\n",
    "can be expressed via the matrix-vector product:\n",
    "\n",
    "$${\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b,$$\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "\n",
    "$$l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2.$$\n",
    "\n",
    "\n",
    "To measure the quality of a model on the entire dataset of $n$ examples,\n",
    "we simply average (or equivalently, sum)\n",
    "the losses on the training set.\n",
    "\n",
    "$$L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.$$\n",
    "\n",
    "When training the model, we want to find parameters ($\\mathbf{w}^*, b^*$)\n",
    "that minimize the total loss across all training examples:\n",
    "\n",
    "$$\\mathbf{w}^*, b^* = \\operatorname*{argmin}_{\\mathbf{w}, b}\\  L(\\mathbf{w}, b).$$\n",
    "\n",
    "\n",
    "### Minibatch Stochastic Gradient Descent\n",
    "\n",
    "In each iteration, we first randomly sample a minibatch $\\mathcal{B}$\n",
    "consisting of a fixed number of training examples.\n",
    "\n",
    "$$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b).$$\n",
    "\n",
    "\n",
    "To summarize, steps of the algorithm are the following:\n",
    "(i) we initialize the values of the model parameters, typically at random;\n",
    "(ii) we iteratively sample random minibatches from the data,\n",
    "updating the parameters in the direction of the negative gradient.\n",
    "For quadratic losses and affine transformations,\n",
    "we can write this out explicitly as follows:\n",
    "\n",
    "$$\\begin{aligned} \\mathbf{w} &\\leftarrow \\mathbf{w} -   \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{\\mathbf{w}} l^{(i)}(\\mathbf{w}, b) = \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right),\\\\ b &\\leftarrow b -  \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_b l^{(i)}(\\mathbf{w}, b)  = b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right). \\end{aligned}$$\n",
    "\n",
    "\n",
    "Note that $\\mathbf{w}$ and $\\mathbf{x}$ are vectors. \n",
    "The set cardinality\n",
    "$|\\mathcal{B}|$ represents\n",
    "the number of examples in each minibatch (the *batch size*)\n",
    "and $\\eta$ denotes the *learning rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27347d5-2b7a-42e4-87ef-c41764b50659",
   "metadata": {
    "origin_pos": 13
   },
   "source": [
    "## The Normal Distribution and Squared Loss\n",
    "\n",
    "The probability density\n",
    "of a normal distribution with mean $\\mu$ and variance $\\sigma^2$ (standard deviation $\\sigma$)\n",
    "is given as\n",
    "\n",
    "$$p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (x - \\mu)^2\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7aac49-1752-4007-a1d8-91e5b16c4726",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "Assume that observations arise from noisy observations,\n",
    "where the noise is normally distributed as follows:\n",
    "\n",
    "$$y = \\mathbf{w}^\\top \\mathbf{x} + b + \\epsilon \\text{ where } \\epsilon \\sim \\mathcal{N}(0, \\sigma^2).$$\n",
    "\n",
    "The *likelihood*\n",
    "of seeing a particular $y$ for a given $\\mathbf{x}$ via\n",
    "\n",
    "$$P(y \\mid \\mathbf{x}) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (y - \\mathbf{w}^\\top \\mathbf{x} - b)^2\\right).$$\n",
    "\n",
    "Now, according to maximum likelihood,\n",
    "the best values of parameters $\\mathbf{w}$ and $b$ are those\n",
    "that maximize the *likelihood* of the entire dataset:\n",
    "\n",
    "$$P(\\mathbf y \\mid \\mathbf X) = \\prod_{i=1}^{n} p(y^{(i)}|\\mathbf{x}^{(i)}).$$\n",
    "\n",
    "We can minimize the *negative log-likelihood*\n",
    "$-\\log P(\\mathbf y \\mid \\mathbf X)$.\n",
    "\n",
    "$$-\\log P(\\mathbf y \\mid \\mathbf X) = \\sum_{i=1}^n \\frac{1}{2} \\log(2 \\pi \\sigma^2) + \\frac{1}{2 \\sigma^2} \\left(y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b\\right)^2.$$\n",
    "\n",
    "Assume $\\sigma$ is some fixed constant.\n",
    "Now the second term is identical to the squared error loss introduced earlier,\n",
    "except for the multiplicative constant $\\frac{1}{\\sigma^2}$.\n",
    "Fortunately, the solution does not depend on $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc38b806-1bce-45b8-82a8-65576a53721f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## From Linear Regression to Deep Networks\n",
    "\n",
    "### Neural Network Diagram\n",
    "\n",
    "![Linear regression is a single-layer neural network.](img/singleneuron.svg)\n",
    "\n",
    "For the neural network,\n",
    "the inputs are $x_1, \\ldots, x_d$,\n",
    "so the *number of inputs* (or *feature dimensionality*) in the input layer is $d$.\n",
    "The output of the network is $o_1$,\n",
    "so the *number of outputs* in the output layer is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcc45ef-af5f-4a94-ad4a-7f930080fbe7",
   "metadata": {
    "origin_pos": 0,
    "tags": []
   },
   "source": [
    "## Classification Problem\n",
    "\n",
    "And since the calculation of each output, $o_1, o_2$, and $o_3$,\n",
    "depends on all inputs, $x_1$, $x_2$, $x_3$, and $x_4$,\n",
    "the output layer of softmax regression can also be described as fully-connected layer.\n",
    "\n",
    "![Softmax regression is a single-layer neural network.](../img/softmaxreg.svg)\n",
    "\n",
    "In vector form\n",
    "$\\mathbf{o} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}$,\n",
    "\n",
    "\n",
    "## Softmax Operation\n",
    "\n",
    "The *softmax function*\n",
    "$$\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o})\\quad \\text{where}\\quad \\hat{y}_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)}. $$\n",
    "\n",
    "\n",
    "## Vectorization for Minibatches\n",
    "\n",
    "Assume that we are given a minibatch $\\mathbf{X}$ of examples\n",
    "with feature dimensionality (number of inputs) $d$ and batch size $n$.\n",
    "Moreover, assume that we have $q$ categories in the output.\n",
    "Then the minibatch features $\\mathbf{X}$ are in $\\mathbb{R}^{n \\times d}$,\n",
    "weights $\\mathbf{W} \\in \\mathbb{R}^{d \\times q}$,\n",
    "and the bias satisfies $\\mathbf{b} \\in \\mathbb{R}^{1\\times q}$.\n",
    "\n",
    "$$ \\begin{aligned} \\mathbf{O} &= \\mathbf{X} \\mathbf{W} + \\mathbf{b}, \\\\ \\hat{\\mathbf{Y}} & = \\mathrm{softmax}(\\mathbf{O}). \\end{aligned} $$\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "Next, we need a loss function to measure\n",
    "the quality of our predicted probabilities.\n",
    "We will rely on maximum likelihood estimation.\n",
    "\n",
    "The softmax function gives us a vector $\\hat{\\mathbf{y}}$,\n",
    "which we can interpret as estimated conditional probabilities\n",
    "of each class given any input $\\mathbf{x}$, e.g.,\n",
    "$\\hat{y}_1$ = $P(y=\\text{cat} \\mid \\mathbf{x})$.\n",
    "\n",
    "$$\n",
    "P(\\mathbf{Y} \\mid \\mathbf{X}) = \\prod_{i=1}^n P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)}).\n",
    "$$\n",
    "\n",
    "According to maximum likelihood estimation,\n",
    "we maximize $P(\\mathbf{Y} \\mid \\mathbf{X})$,\n",
    "which is\n",
    "equivalent to minimizing the negative log-likelihood:\n",
    "\n",
    "$$\n",
    "-\\log P(\\mathbf{Y} \\mid \\mathbf{X}) = \\sum_{i=1}^n -\\log P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)})\n",
    "= \\sum_{i=1}^n l(\\mathbf{y}^{(i)}, \\hat{\\mathbf{y}}^{(i)}),\n",
    "$$\n",
    "\n",
    "where for any pair of label $\\mathbf{y}$ and model prediction $\\hat{\\mathbf{y}}$ over $q$ classes,\n",
    "the loss function $l$ is\n",
    "\n",
    "$$ l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^q y_j \\log \\hat{y}_j. $$\n",
    "\n",
    "\n",
    "### Softmax and Derivatives\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "l(\\mathbf{y}, \\hat{\\mathbf{y}}) &=  - \\sum_{j=1}^q y_j \\log \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} \\\\\n",
    "&= \\sum_{j=1}^q y_j \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j\\\\\n",
    "&= \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To understand a bit better what is going on,\n",
    "consider the derivative with respect to any logit $o_j$. We get\n",
    "\n",
    "$$\n",
    "\\partial_{o_j} l(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} - y_j = \\mathrm{softmax}(\\mathbf{o})_j - y_j.\n",
    "$$\n",
    "\n",
    "In other words, the derivative is the difference\n",
    "between the probability assigned by our model,\n",
    "as expressed by the softmax operation,\n",
    "and what actually happened, as expressed by elements in the one-hot label vector.\n",
    "\n",
    "\n",
    "### Entropy\n",
    "\n",
    "In information theory, this quantity is called the *entropy* of a distribution $P$,\n",
    "and it is captured by the following equation:\n",
    "\n",
    "$$H[P] = \\sum_j - P(j) \\log P(j).$$\n",
    "\n",
    "\n",
    "### Cross-Entropy Revisited\n",
    "\n",
    "In short, we can think of the cross-entropy classification objective\n",
    "in two ways: (i) as maximizing the likelihood of the observed data;\n",
    "and (ii) as minimizing our surprisal (and thus the number of bits)\n",
    "required to communicate the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee57dad-3947-48e6-a09e-ff57f912a910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
